(()=>{var e={2528:(e,t,s)=>{"use strict";var a=s(1430);async function o(){try{if(!navigator.gpu)throw Error("WebGPU is not supported by this browser. Use openai model in the setting");if(!await navigator.gpu.requestAdapter())throw Error("WebGPU is not supported (no adapter found)");self.postMessage({status:"webgpu_check_success",data:"WebGPU is available."})}catch(e){self.postMessage({status:"error",type:"webgpu_check_error",data:e.toString()})}}a._K2.allowLocalModels=!1;class n{static async getInstance(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:null;if("undefined"!=typeof navigator&&navigator.gpu)try{if(!await navigator.gpu.requestAdapter()){let e="WebGPU is not supported (no adapter found). Local model disabled.";throw self.postMessage({status:"error",type:"webgpu_unavailable",data:e}),Error(e)}}catch(t){let e="Error requesting WebGPU adapter: ".concat(t.message,". Local model disabled.");throw self.postMessage({status:"error",type:"webgpu_unavailable",data:e}),Error(e)}else{let e="WebGPU is not supported by this browser. Local model disabled.";throw self.postMessage({status:"error",type:"webgpu_unavailable",data:e}),Error(e)}return this.tokenizer||(this.tokenizer=a.v6I.from_pretrained(this.model_id,{progress_callback:e})),this.model||(this.model=a.W6E.from_pretrained(this.model_id,{dtype:"q4f16",device:"webgpu",progress_callback:e})),[this.tokenizer,this.model]=await Promise.all([this.tokenizer,this.model]),[this.tokenizer,this.model]}}n.model_id="onnx-community/Qwen3-0.6B-ONNX",n.tokenizer=null,n.model=null;let r=new a.dYU,i=null;async function l(e){let{transcriptionText:t,meetingTitle:s,settings:a}=e;self.postMessage({status:"summarizing_started",data:{reasonEnabled:!1}});let{apiToken:o,baseUrl:n,modelName:r}=a.llm.openai;if(!o)return void self.postMessage({status:"error",type:"summarization_error",data:"OpenAI API token is not configured."});let i="Summarize the following meeting transcript concisely, focusing on key decisions, action items, and important discussions.";s&&""!==s.trim()&&(i+=' The title of this meeting is "'.concat(s,'".'));let l=[{role:"system",content:"You are a helpful meeting summarization assistant. Provide a concise summary based on the provided transcript."},{role:"user",content:i+=' Structure the summary with the following sections:\n\n1. Meeting Overview – Date, attendees, and primary objective.\n2. Key Discussion Points – Bullet points of major topics covered (limit to 3-5).\n3. Decisions Made – Clear outcomes or resolutions agreed upon.\n4. Action Items – Specific tasks, assigned owners, and deadlines (if mentioned).\n5. Next Steps – Any follow-up meetings or pending discussions.\nMaintain a professional tone, avoid unnecessary details, and ensure clarity. Here is the transcript:\n\n"'.concat(t,'"')}];try{let e=await fetch("".concat(n,"/chat/completions"),{method:"POST",headers:{"Content-Type":"application/json",Authorization:"Bearer ".concat(o)},body:JSON.stringify({model:r,messages:l,stream:!0})});if(!e.ok){var c;let t=await e.json().catch(()=>({message:e.statusText}));throw Error("OpenAI API Error: ".concat(e.status," ").concat(t.message||(null==(c=t.error)?void 0:c.message)||""))}let t=e.body.getReader(),s=new TextDecoder,a="",i="";for(;;){let{done:e,value:o}=await t.read();if(e)break;let n=(a+=s.decode(o,{stream:!0})).indexOf("\n\n");for(;-1!==n;){let e=a.substring(0,n);if(a=a.substring(n+2),e.startsWith("data: ")){let t=e.substring(6);if("[DONE]"===t.trim())break;try{let e=JSON.parse(t);if(e.choices&&e.choices[0].delta&&e.choices[0].delta.content){let t=e.choices[0].delta.content;i+=t,self.postMessage({status:"update",output:t,tps:null,numTokens:null,state:"answering"})}}catch(e){console.warn("Summarization worker: Could not parse OpenAI stream chunk:",t,e)}}n=a.indexOf("\n\n")}if(a.includes("[DONE]"))break}self.postMessage({status:"summarizing_complete",output:i})}catch(e){console.error("OpenAI Summarization Error:",e),self.postMessage({status:"error",type:"summarization_error",data:e.toString()})}}async function c(e){let{transcriptionText:t,meetingTitle:s,settings:a}=e;if(a&&a.llm&&"openai"===a.llm.type)return void await l({transcriptionText:t,meetingTitle:s,settings:a});await u({transcriptionText:t,meetingTitle:s,settings:a})}async function u(e){let{transcriptionText:t,meetingTitle:s}=e;self.postMessage({status:"summarizing_started",data:{reasonEnabled:!0}});try{let e,o,l,[c,u]=await n.getInstance(e=>{self.postMessage(e)}),p="Summarize the following meeting transcript concisely, focusing on key decisions, action items, and important discussions.";s&&""!==s.trim()&&(p+=' The title of this meeting is "'.concat(s,'".')),p+=' Structure the summary with the following sections:\n\n1. Meeting Overview – Date, attendees, and primary objective.\n2. Key Discussion Points – Bullet points of major topics covered (limit to 3-5).\n3. Decisions Made – Clear outcomes or resolutions agreed upon.\n4. Action Items – Specific tasks, assigned owners, and deadlines (if mentioned).\n5. Next Steps – Any follow-up meetings or pending discussions.\nMaintain a professional tone, avoid unnecessary details, and ensure clarity. Here is the transcript:\n\n"'.concat(t,'"');let d=[{role:"system",content:"You are a helpful meeting summarization assistant. First, think about the key points of the transcription, then provide a concise summary."},{role:"user",content:p}],m=c.apply_chat_template(d,{add_generation_prompt:!0,return_dict:!0}),[f,g]=c.encode("<think></think>",{add_special_tokens:!1}),h=0,y="thinking",b=new a.T1P(c,{skip_prompt:!0,skip_special_tokens:!0,callback_function:e=>{let t={status:"update",output:"",tps:o,numTokens:h,state:y};"answering"===y&&(t.output=e),self.postMessage(t)},token_callback_function:t=>{switch(null!=e||(e=performance.now()),h++>0&&(o=h/(performance.now()-e)*1e3),Number(t[0])){case f:y="thinking";break;case g:y="answering"}}}),_={...m,past_key_values:i,do_sample:!0,top_k:20,temperature:.6,max_new_tokens:1024,streamer:b,stopping_criteria:r,return_dict_in_generate:!0};void 0===m.token_type_ids&&delete _.token_type_ids;let{past_key_values:w,sequences:k}=await u.generate(_);i=w;let v=c.batch_decode(k,{skip_special_tokens:!0})[0],M=c.decode(m.input_ids,{skip_special_tokens:!0});v.startsWith(M)?l=v.substring(M.length):(l=v,console.warn("Summarization worker: Prompt (from input_ids) not found at the start of batch_decode output. The final summary might contain parts of the prompt.")),l=l.replace(RegExp("<think>.*?<\\/think>","gs"),"").trim(),self.postMessage({status:"summarizing_complete",output:l})}catch(e){self.postMessage({status:"error",type:"summarization_error",data:e.toString()})}}async function p(){self.postMessage({status:"loading",data:"Loading summarization model..."});try{let[e,t]=await n.getInstance(e=>{self.postMessage(e)});self.postMessage({status:"loading",data:"Compiling shaders and warming up summarization model..."});let s=e("This is a test.");await t.generate({...s,max_new_tokens:1}),self.postMessage({status:"summarization_ready"})}catch(e){self.postMessage({status:"error",type:"load_model_error",data:e.toString()})}}self.addEventListener("message",async e=>{let{type:t,data:s}=e.data;switch(t){case"check_webgpu":o();break;case"load_model":p();break;case"summarize":r.reset(),c(s);break;case"interrupt_summarization":r.interrupt(),i=null,self.postMessage({status:"summarization_interrupted"});break;case"reset_summarization":i=null,r.reset(),self.postMessage({status:"summarization_reset"});break;default:console.warn("Unknown message type received in summarization worker:",t)}})}},t={};function s(a){var o=t[a];if(void 0!==o)return o.exports;var n=t[a]={exports:{}},r=!0;try{e[a](n,n.exports,s),r=!1}finally{r&&delete t[a]}return n.exports}s.m=e,s.x=()=>{var e=s.O(void 0,[803,643,699],()=>s(2528));return s.O(e)},(()=>{var e=[];s.O=(t,a,o,n)=>{if(a){n=n||0;for(var r=e.length;r>0&&e[r-1][2]>n;r--)e[r]=e[r-1];e[r]=[a,o,n];return}for(var i=1/0,r=0;r<e.length;r++){for(var[a,o,n]=e[r],l=!0,c=0;c<a.length;c++)(!1&n||i>=n)&&Object.keys(s.O).every(e=>s.O[e](a[c]))?a.splice(c--,1):(l=!1,n<i&&(i=n));if(l){e.splice(r--,1);var u=o();void 0!==u&&(t=u)}}return t}})(),s.d=(e,t)=>{for(var a in t)s.o(t,a)&&!s.o(e,a)&&Object.defineProperty(e,a,{enumerable:!0,get:t[a]})},s.f={},s.e=e=>Promise.all(Object.keys(s.f).reduce((t,a)=>(s.f[a](e,t),t),[])),s.u=e=>"static/chunks/"+(({643:"037f3a08",803:"adda3502"})[e]||e)+"."+({643:"b3a221f1a03a90d7",699:"4db0c599526b17b2",803:"eba37b97483c5080"})[e]+".js",s.miniCssF=e=>{},s.g=function(){if("object"==typeof globalThis)return globalThis;try{return this||Function("return this")()}catch(e){if("object"==typeof window)return window}}(),s.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),s.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},s.U=function(e){var t=new URL(e,"x:/"),s={};for(var a in t)s[a]=t[a];for(var a in s.href=e,s.pathname=e.replace(/[?#].*/,""),s.origin=s.protocol="",s.toString=s.toJSON=()=>e,s)Object.defineProperty(this,a,{enumerable:!0,configurable:!0,value:s[a]})},s.U.prototype=URL.prototype,(()=>{var e;s.tt=()=>(void 0===e&&(e={createScriptURL:e=>e},"undefined"!=typeof trustedTypes&&trustedTypes.createPolicy&&(e=trustedTypes.createPolicy("nextjs#bundler",e))),e)})(),s.tu=e=>s.tt().createScriptURL(e),s.p="/noted-pak/_next/",(()=>{var e={528:1};s.f.i=(t,a)=>{e[t]||importScripts(s.tu(s.p+s.u(t)))};var t=self.webpackChunk_N_E=self.webpackChunk_N_E||[],a=t.push.bind(t);t.push=t=>{var[o,n,r]=t;for(var i in n)s.o(n,i)&&(s.m[i]=n[i]);for(r&&r(s);o.length;)e[o.pop()]=1;a(t)}})(),(()=>{var e=s.x;s.x=()=>Promise.all([803,643,699].map(s.e,s)).then(e)})(),_N_E=s.x()})();