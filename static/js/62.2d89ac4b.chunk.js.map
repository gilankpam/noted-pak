{"version":3,"file":"static/js/62.2d89ac4b.chunk.js","mappings":"+DAmBA,MAAMA,EAOJ,wBAAaC,GAAuC,IAA3BC,EAAiBC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,KAM3C,OALKG,KAAKC,kBACRD,KAAKC,gBAAkBD,KAAKE,cAAcN,IAIrCI,KAAKC,eACd,CAEA,0BAAaC,CAAcN,GAmBzB,OAjBAI,KAAKG,gBAAkBC,EAAAA,IAAcC,gBAAgBL,KAAKM,SAAU,CAClEV,sBAEFI,KAAKO,gBAAkBC,EAAAA,IAAcH,gBAAgBL,KAAKM,SAAU,CAClEV,sBAEFI,KAAKS,YAAcC,EAAAA,IAAgCL,gBACjDL,KAAKM,SACL,CACEK,MAAO,CACLC,cAAe,OACfC,qBAAsB,MAExBC,OAAQ,SACRlB,sBAGG,CAACI,KAAKG,UAAWH,KAAKO,UAAWP,KAAKS,MAC/C,EApCIf,EACGY,SAAW,8BADdZ,EAEGS,UAAY,KAFfT,EAGGa,UAAY,KAHfb,EAIGe,MAAQ,KAJXf,EAKGO,gBAAkB,KAkC3B,IAAIc,GAAa,EAqHjBC,KAAKC,iBAAiB,WAAWC,UAC/B,MAAM,KAAEC,EAAI,KAAEC,GAASC,EAAED,KAEzB,OAAQD,GACN,IAAK,aAxCTD,iBACEF,KAAKM,YAAY,CACfC,OAAQ,UACRH,KAAM,gCAGR,IAEE,MAAO,CAAEb,EAAWE,SACZf,EAAmCC,aAAa6B,IACpDR,KAAKM,YAAYE,MAGrBR,KAAKM,YAAY,CACfC,OAAQ,UACRH,KAAM,8CAQR,MAAMK,EAAa,IAAIC,aAAa,MAC9BC,QAAuBpB,EAAUkB,EAAY,CAAEG,cAAe,aAE9DnB,EAAMoB,SAAS,CACnBC,eAAgBH,EAAeG,eAC/BC,eAAgB,IAElBf,KAAKM,YAAY,CAAEC,OAAQ,SAC7B,CAAE,MAAOS,GACPhB,KAAKM,YAAY,CAAEC,OAAQ,QAASS,MAAM,6BAADC,OAA+BD,EAAME,SAAWC,MAAOH,EAAMG,OACxG,CACF,CAOYC,GACN,MAEF,IAAK,iBA5HTlB,eAAuBmB,GAAuB,IAAtB,MAAEC,EAAK,SAAEC,GAAUF,EACzC,GAAItB,EACFC,KAAKM,YAAY,CAAEC,OAAQ,OAAQS,MAAO,2BAD5C,CAIAjB,GAAa,EAEbC,KAAKM,YAAY,CAAEC,OAAQ,UAE3B,IACE,MAAOpB,EAAWI,EAAWE,SACrBf,EAAmCC,cAE3C,IAAKQ,IAAcI,IAAcE,EAC7B,MAAM,IAAI+B,MAAM,gCAGpB,IAAIC,EACAC,EAAY,EACZC,EAAM,EAEV,MAAMC,EAA0BA,KAAO,IAADC,EAC3B,QAATA,EAAAJ,SAAS,IAAAI,IAATJ,EAAcK,YAAYC,OAC1BL,IACIA,EAAY,GAAKD,IACnBE,EAAOD,GAAaI,YAAYC,MAAQN,GAAc,MAIpDO,EAAqBC,IACzBjC,KAAKM,YAAY,CACfC,OAAQ,SACR0B,SACAN,MACAD,eAIEQ,EAAW,IAAIC,EAAAA,IAAahD,EAAW,CAC3CiD,aAAa,EACbC,qBAAqB,EACrBL,oBACAJ,4BAKIU,QAAe/C,EAAU+B,EAAO,CAAEV,cAAe,aAGjDnB,EAAMoB,UAAQ0B,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAC,CAAC,EACjBD,GAAM,IACTvB,eAjGiB,GAkGjBQ,SAAUA,GAAY,KACtBW,cAaFlC,KAAKM,YAAY,CACfC,OAAQ,YAKZ,CAAE,MAAOS,GACPhB,KAAKM,YAAY,CAAEC,OAAQ,QAASS,MAAOA,EAAME,QAASC,MAAOH,EAAMG,OACzE,CAAC,QACCpB,GAAa,CACf,CAzEA,CA0EF,CA+CYc,CAAST,GACf,MACF,QAEEoC,QAAQC,KAAK,yCAADxB,OAA0Cd,O,GC3LxDuC,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqB7D,IAAjB8D,EACH,OAAOA,EAAaC,QAGrB,IAAIC,EAASL,EAAyBE,GAAY,CAGjDE,QAAS,CAAC,GAOX,OAHAE,EAAoBJ,GAAUG,EAAQA,EAAOD,QAASH,GAG/CI,EAAOD,OACf,CAGAH,EAAoBM,EAAID,EAGxBL,EAAoBnC,EAAI,KAGvB,IAAI0C,EAAsBP,EAAoBQ,OAAEpE,EAAW,CAAC,IAAI,MAAM,IAAO4D,EAAoB,MAEjG,OADAO,EAAsBP,EAAoBQ,EAAED,I,MChC7C,IAAIE,EAAW,GACfT,EAAoBQ,EAAI,CAACE,EAAQC,EAAUC,EAAIC,KAC9C,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASC,EAAI,EAAGA,EAAIP,EAAStE,OAAQ6E,IAAK,CACrCL,EAAWF,EAASO,GAAG,GACvBJ,EAAKH,EAASO,GAAG,GACjBH,EAAWJ,EAASO,GAAG,GAE3B,IAJA,IAGIC,GAAY,EACPC,EAAI,EAAGA,EAAIP,EAASxE,OAAQ+E,MACpB,EAAXL,GAAsBC,GAAgBD,IAAaM,OAAOC,KAAKpB,EAAoBQ,GAAGa,OAAOC,GAAStB,EAAoBQ,EAAEc,GAAKX,EAASO,MAC9IP,EAASY,OAAOL,IAAK,IAErBD,GAAY,EACTJ,EAAWC,IAAcA,EAAeD,IAG7C,GAAGI,EAAW,CACbR,EAASc,OAAOP,IAAK,GACrB,IAAIQ,EAAIZ,SACExE,IAANoF,IAAiBd,EAASc,EAC/B,CACD,CACA,OAAOd,CArBP,CAJCG,EAAWA,GAAY,EACvB,IAAI,IAAIG,EAAIP,EAAStE,OAAQ6E,EAAI,GAAKP,EAASO,EAAI,GAAG,GAAKH,EAAUG,IAAKP,EAASO,GAAKP,EAASO,EAAI,GACrGP,EAASO,GAAK,CAACL,EAAUC,EAAIC,G,KCJ/Bb,EAAoByB,EAAI,CAACtB,EAASuB,KACjC,IAAI,IAAIJ,KAAOI,EACX1B,EAAoB2B,EAAED,EAAYJ,KAAStB,EAAoB2B,EAAExB,EAASmB,IAC5EH,OAAOS,eAAezB,EAASmB,EAAK,CAAEO,YAAY,EAAMC,IAAKJ,EAAWJ,MCJ3EtB,EAAoB+B,EAAI,CAAC,EAGzB/B,EAAoBtC,EAAKsE,GACjBC,QAAQC,IAAIf,OAAOC,KAAKpB,EAAoB+B,GAAGI,QAAO,CAACC,EAAUd,KACvEtB,EAAoB+B,EAAET,GAAKU,EAASI,GAC7BA,IACL,KCNJpC,EAAoBqC,EAAKL,GAEjB,aAAeA,EAAU,IAAM,CAAC,IAAM,WAAW,IAAM,YAAYA,GAAW,YCFtFhC,EAAoBsC,SAAYN,MCDhChC,EAAoB2B,EAAI,CAACY,EAAKC,IAAUrB,OAAOsB,UAAUC,eAAeC,KAAKJ,EAAKC,GCClFxC,EAAoBwB,EAAKrB,IACH,qBAAXyC,QAA0BA,OAAOC,aAC1C1B,OAAOS,eAAezB,EAASyC,OAAOC,YAAa,CAAEC,MAAO,WAE7D3B,OAAOS,eAAezB,EAAS,aAAc,CAAE2C,OAAO,KCLvD9C,EAAoB+C,EAAI,c,MCAxB/C,EAAoBgD,EAAI3F,KAAK4F,SAAW,aAIxC,IAAIC,EAAkB,CACrB,GAAI,GAkBLlD,EAAoB+B,EAAEf,EAAI,CAACgB,EAASI,KAE/Bc,EAAgBlB,IAElBmB,cAAcnD,EAAoB+C,EAAI/C,EAAoBqC,EAAEL,KAK/D,IAAIoB,EAAqB/F,KAA4B,sBAAIA,KAA4B,uBAAK,GACtFgG,EAA6BD,EAAmBE,KAAKC,KAAKH,GAC9DA,EAAmBE,KAzBC7F,IACnB,IAAIkD,EAAWlD,EAAK,GAChB+F,EAAc/F,EAAK,GACnBgG,EAAUhG,EAAK,GACnB,IAAI,IAAIwC,KAAYuD,EAChBxD,EAAoB2B,EAAE6B,EAAavD,KACrCD,EAAoBM,EAAEL,GAAYuD,EAAYvD,IAIhD,IADGwD,GAASA,EAAQzD,GACdW,EAASxE,QACd+G,EAAgBvC,EAAS+C,OAAS,EACnCL,EAA2B5F,G,WCrB5B,IAAIkG,EAAO3D,EAAoBnC,EAC/BmC,EAAoBnC,EAAI,IAChBoE,QAAQC,IAAI,CAClBlC,EAAoBtC,EAAE,KACtBsC,EAAoBtC,EAAE,OACpBkG,KAAKD,E,KCJiB3D,EAAoBnC,G","sources":["transcription.worker.js","../webpack/bootstrap","../webpack/runtime/chunk loaded","../webpack/runtime/define property getters","../webpack/runtime/ensure chunk","../webpack/runtime/get javascript chunk filename","../webpack/runtime/get mini-css chunk filename","../webpack/runtime/hasOwnProperty shorthand","../webpack/runtime/make namespace object","../webpack/runtime/publicPath","../webpack/runtime/importScripts chunk loading","../webpack/runtime/startup chunk dependencies","../webpack/startup"],"sourcesContent":["/* eslint-disable no-restricted-globals */\n/* eslint-env worker */\n/* global self */ // This line might now be redundant if no-restricted-globals is disabled, but harmless to keep.\nimport {\n  AutoTokenizer,\n  AutoProcessor,\n  WhisperForConditionalGeneration,\n  TextStreamer,\n  // env, // env was unused, removing for now. If tensor creation needs it, it can be re-added.\n} from \"@huggingface/transformers\";\n\n// If 'full' is not a direct export, we might need to create tensors like:\n// new env.Tensor('float32', new Float32Array(1 * 80 * 3000).fill(0.0), [1, 80, 3000]);\n\nconst MAX_NEW_TOKENS = 64; // Increased from 64\n\n/**\n * This class uses the Singleton pattern to ensure that only one instance of the model is loaded.\n */\nclass AutomaticSpeechRecognitionPipeline {\n  static model_id = \"onnx-community/whisper-base\"; // Using the same model as before\n  static tokenizer = null;\n  static processor = null;\n  static model = null;\n  static instancePromise = null; // To handle concurrent getInstance calls\n\n  static async getInstance(progress_callback = null) {\n    if (!this.instancePromise) {\n      this.instancePromise = this.loadResources(progress_callback);\n    }\n    // If already loading, wait for the existing promise to resolve.\n    // If loaded, this will return the resolved promise with [tokenizer, processor, model].\n    return this.instancePromise;\n  }\n\n  static async loadResources(progress_callback) {\n    // Ensure components are loaded sequentially or awaited properly\n    this.tokenizer = await AutoTokenizer.from_pretrained(this.model_id, {\n      progress_callback,\n    });\n    this.processor = await AutoProcessor.from_pretrained(this.model_id, {\n      progress_callback,\n    });\n    this.model = await WhisperForConditionalGeneration.from_pretrained(\n      this.model_id,\n      {\n        dtype: {\n          encoder_model: \"fp32\",\n          decoder_model_merged: \"q4\",\n        },\n        device: \"webgpu\",\n        progress_callback,\n      }\n    );\n    return [this.tokenizer, this.processor, this.model];\n  }\n}\n\nlet processing = false;\nasync function generate({ audio, language }) {\n  if (processing) {\n    self.postMessage({ status: \"busy\", error: \"Processor is busy.\" });\n    return;\n  }\n  processing = true;\n\n  self.postMessage({ status: \"start\" });\n\n  try {\n    const [tokenizer, processor, model] =\n      await AutomaticSpeechRecognitionPipeline.getInstance();\n\n    if (!tokenizer || !processor || !model) {\n        throw new Error(\"Model components not loaded.\");\n    }\n\n    let startTime;\n    let numTokens = 0;\n    let tps = 0; // Initialize tps\n\n    const token_callback_function = () => {\n      startTime ??= performance.now();\n      numTokens++;\n      if (numTokens > 1 && startTime) { // Avoid division by zero and ensure startTime is set\n        tps = (numTokens / (performance.now() - startTime)) * 1000;\n      }\n    };\n\n    const callback_function = (output) => {\n      self.postMessage({\n        status: \"update\",\n        output, // This is usually a list of generated token IDs or a string part\n        tps,\n        numTokens,\n      });\n    };\n\n    const streamer = new TextStreamer(tokenizer, {\n      skip_prompt: true,\n      skip_special_tokens: true, // This is good\n      callback_function,\n      token_callback_function,\n    });\n\n    // The audio here is expected to be a Float32Array, resampled to 16kHz\n    // The processor will convert it to input_features\n    const inputs = await processor(audio, { sampling_rate: 16000 });\n\n    // const outputs = // This variable was unused as TextStreamer handles output.\n    await model.generate({\n      ...inputs,\n      max_new_tokens: MAX_NEW_TOKENS,\n      language: language || \"en\", // Default to English if no language specified\n      streamer,\n      // Other common parameters for Whisper if needed:\n      // num_beams: 1, // For greedy decoding, faster\n      // do_sample: false, // For greedy decoding\n      // temperature: 0, // For greedy decoding\n      // task: 'transcribe', // Explicitly set task\n    });\n\n    // After streaming, TextStreamer's callback_function handles intermediate outputs.\n    // The final decoded output is not typically needed here if streamer is used for all updates.\n    // If a final full string is desired from 'complete', it would be assembled by the main thread\n    // or TextStreamer could have a method to get the full string.\n\n    self.postMessage({\n      status: \"complete\",\n      // No final 'output' here as streamer sends updates.\n      // App.js will accumulate the text from 'update' messages.\n    });\n\n  } catch (error) {\n    self.postMessage({ status: \"error\", error: error.message, stack: error.stack });\n  } finally {\n    processing = false;\n  }\n}\n\nasync function load() {\n  self.postMessage({\n    status: \"loading\",\n    data: \"Loading model components...\",\n  });\n\n  try {\n    // const [tokenizer, processor, model] = // tokenizer is not used in this scope\n    const [, processor, model] = // processor and model are used\n      await AutomaticSpeechRecognitionPipeline.getInstance((x) => {\n        self.postMessage(x); // Forward progress events\n      });\n\n    self.postMessage({\n      status: \"loading\",\n      data: \"Compiling shaders and warming up model...\",\n    });\n    \n    // Create a dummy Float32Array for warmup input\n    // Whisper expects 16kHz mono audio. 30s = 16000 * 30 = 480000 samples.\n    // The processor converts this to input_features (mel spectrogram).\n    // The dummy input for model.generate should be `input_features`.\n    // The processor output is an object like { input_features: Tensor, ... }\n    const dummyAudio = new Float32Array(16000 * 1); // 1 second of silence for warmup\n    const dummyProcessed = await processor(dummyAudio, { sampling_rate: 16000 });\n\n    await model.generate({\n      input_features: dummyProcessed.input_features, // Use processed dummy audio\n      max_new_tokens: 1, // Generate only one token for warmup\n    });\n    self.postMessage({ status: \"ready\" });\n  } catch (error) {\n    self.postMessage({ status: \"error\", error: `Model load/warmup failed: ${error.message}`, stack: error.stack });\n  }\n}\n\nself.addEventListener(\"message\", async (e) => {\n  const { type, data } = e.data;\n\n  switch (type) {\n    case \"load\":\n      await load(); // Ensure load is awaited if it's async and can throw\n      break;\n\n    case \"generate\":\n      await generate(data); // Ensure generate is awaited\n      break;\n    default:\n      // Optional: handle unknown message types\n      console.warn(`Worker received unknown message type: ${type}`);\n      break;\n  }\n});\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n// the startup function\n__webpack_require__.x = () => {\n\t// Load entry module and return exports\n\t// This entry module depends on other loaded chunks and execution need to be delayed\n\tvar __webpack_exports__ = __webpack_require__.O(undefined, [827,778], () => (__webpack_require__(62)))\n\t__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n\treturn __webpack_exports__;\n};\n\n","var deferred = [];\n__webpack_require__.O = (result, chunkIds, fn, priority) => {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every((key) => (__webpack_require__.O[key](chunkIds[j])))) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = (chunkId) => {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce((promises, key) => {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.u = (chunkId) => {\n\t// return url for filenames based on template\n\treturn \"static/js/\" + chunkId + \".\" + {\"778\":\"bf74655f\",\"827\":\"d8841902\"}[chunkId] + \".chunk.js\";\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.miniCssF = (chunkId) => {\n\t// return url for filenames based on template\n\treturn undefined;\n};","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.p = \"/noted-pak/\";","__webpack_require__.b = self.location + \"/../../../\";\n\n// object to store loaded chunks\n// \"1\" means \"already loaded\"\nvar installedChunks = {\n\t62: 1\n};\n\n// importScripts chunk loading\nvar installChunk = (data) => {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\twhile(chunkIds.length)\n\t\tinstalledChunks[chunkIds.pop()] = 1;\n\tparentChunkLoadingFunction(data);\n};\n__webpack_require__.f.i = (chunkId, promises) => {\n\t// \"1\" is the signal for \"already loaded\"\n\tif(!installedChunks[chunkId]) {\n\t\tif(true) { // all chunks have JS\n\t\t\timportScripts(__webpack_require__.p + __webpack_require__.u(chunkId));\n\t\t}\n\t}\n};\n\nvar chunkLoadingGlobal = self[\"webpackChunknoted_pak\"] = self[\"webpackChunknoted_pak\"] || [];\nvar parentChunkLoadingFunction = chunkLoadingGlobal.push.bind(chunkLoadingGlobal);\nchunkLoadingGlobal.push = installChunk;\n\n// no HMR\n\n// no HMR manifest","var next = __webpack_require__.x;\n__webpack_require__.x = () => {\n\treturn Promise.all([\n\t\t__webpack_require__.e(827),\n\t\t__webpack_require__.e(778)\n\t]).then(next);\n};","// run startup\nvar __webpack_exports__ = __webpack_require__.x();\n"],"names":["AutomaticSpeechRecognitionPipeline","getInstance","progress_callback","arguments","length","undefined","this","instancePromise","loadResources","tokenizer","AutoTokenizer","from_pretrained","model_id","processor","AutoProcessor","model","WhisperForConditionalGeneration","dtype","encoder_model","decoder_model_merged","device","processing","self","addEventListener","async","type","data","e","postMessage","status","x","dummyAudio","Float32Array","dummyProcessed","sampling_rate","generate","input_features","max_new_tokens","error","concat","message","stack","load","_ref","audio","language","Error","startTime","numTokens","tps","token_callback_function","_startTime","performance","now","callback_function","output","streamer","TextStreamer","skip_prompt","skip_special_tokens","inputs","_objectSpread","console","warn","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","exports","module","__webpack_modules__","m","__webpack_exports__","O","deferred","result","chunkIds","fn","priority","notFulfilled","Infinity","i","fulfilled","j","Object","keys","every","key","splice","r","d","definition","o","defineProperty","enumerable","get","f","chunkId","Promise","all","reduce","promises","u","miniCssF","obj","prop","prototype","hasOwnProperty","call","Symbol","toStringTag","value","p","b","location","installedChunks","importScripts","chunkLoadingGlobal","parentChunkLoadingFunction","push","bind","moreModules","runtime","pop","next","then"],"sourceRoot":""}