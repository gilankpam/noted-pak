{"version":3,"file":"static/js/575.2bacd982.chunk.js","mappings":"iEAqCA,MAAMA,EAKJ,wBAAaC,GAAuC,IAA3BC,EAAiBC,UAAAC,OAAA,QAAAC,IAAAF,UAAA,GAAAA,UAAA,GAAG,KAiB3C,OAhBKG,KAAKC,YACRD,KAAKC,UAAYC,EAAAA,IAAcC,gBAAgBH,KAAKI,SAAU,CAC5DR,uBAICI,KAAKK,QACRL,KAAKK,MAAQC,EAAAA,IAAqBH,gBAAgBH,KAAKI,SAAU,CAC/DG,MAAO,QACPC,OAAQ,SACRZ,wBAKHI,KAAKC,UAAWD,KAAKK,aAAeI,QAAQC,IAAI,CAACV,KAAKC,UAAWD,KAAKK,QAChE,CAACL,KAAKC,UAAWD,KAAKK,MAC/B,EAvBIX,EACGU,SAAW,iCADdV,EAEGO,UAAY,KAFfP,EAGGW,MAAQ,KAuBjB,MAAMM,EAAoB,IAAIC,EAAAA,IAC9B,IAAIC,EAAwB,KA4K5BC,KAAKC,iBAAiB,WAAWC,UAC/B,MAAM,KAAEC,EAAI,KAAEC,GAASC,EAAED,KAEzB,OAAQD,GACN,IAAK,gBArOTD,iBACE,IACE,IAAKI,UAAUC,IACb,MAAM,IAAIC,MAAM,4CAGlB,UADsBF,UAAUC,IAAIE,iBAElC,MAAM,IAAID,MAAM,8CAGlBR,KAAKU,YAAY,CACfC,OAAQ,uBACRP,KAAM,wBAEV,CAAE,MAAOC,GACPL,KAAKU,YAAY,CACfC,OAAQ,QACRR,KAAM,qBACNC,KAAMC,EAAEO,YAEZ,CACF,CAiNMC,GACA,MAEF,IAAK,cAvCTX,iBACEF,KAAKU,YAAY,CACfC,OAAQ,UACRP,KAAM,mCAGR,IACE,MAAOjB,EAAWI,SAAeX,EAAsBC,aAAaiC,IAClEd,KAAKU,YAAYI,MAGnBd,KAAKU,YAAY,CACfC,OAAQ,UACRP,KAAM,4DAIR,MACMW,EAAS5B,EADK,yBAEdI,EAAMyB,UAAQC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAC,CAAC,EAAIF,GAAM,IAAEG,eAAgB,KAClDlB,KAAKU,YAAY,CAAEC,OAAQ,uBAC7B,CAAE,MAAON,GACPL,KAAKU,YAAY,CACfC,OAAQ,QACRR,KAAM,mBACNC,KAAMC,EAAEO,YAEZ,CACF,CAYMO,GACA,MAEF,IAAK,YACHtB,EAAkBuB,QAtLxBlB,eAA8BmB,GAAuC,IAAtC,kBAAEC,EAAiB,aAAEC,GAAcF,EAChErB,KAAKU,YAAY,CAAEC,OAAQ,sBAAuBP,KAAM,CAAEoB,eAAe,KAEzE,IACE,MAAOrC,EAAWI,SAAeX,EAAsBC,aAAa4C,IAClEzB,KAAKU,YAAYe,MAInB,IAAIC,EAAW,4HACXH,GAAwC,KAAxBA,EAAaI,SAC/BD,GAAW,kCAAAE,OAAsCL,EAAY,OAE/DG,GAAW,+hBAAAE,OAA0gBN,EAAiB,KAGtiB,MAAMO,EAAW,CACf,CAAEC,KAAM,SAAUC,QAAS,8IAC3B,CAAED,KAAM,OAAQC,QAASL,IAG3BM,QAAQC,IAAIJ,GACZ,MAAMd,EAAS5B,EAAU+C,oBAAoBL,EAAU,CACrDM,uBAAuB,EACvBC,aAAa,KAIRC,EAAyBC,GAAyBnD,EAAUoD,OACjE,kBACA,CAAEC,oBAAoB,IAGxB,IAAIC,EAEAC,EADAC,EAAY,EAEZC,EAAQ,YAEZ,MAAMC,EAA2BC,IAAY,IAADC,EAM1C,OALS,QAATA,EAAAN,SAAS,IAAAM,IAATN,EAAcO,YAAYC,OACtBN,IAAc,IAChBD,EAAOC,GAAaK,YAAYC,MAAQR,GAAc,KAGhDS,OAAOJ,EAAO,KACpB,KAAKT,EACHO,EAAQ,WACR,MACF,KAAKN,EACHM,EAAQ,cAMRO,EAAqBC,IACzB,IAAIC,EAAa,CACf1C,OAAQ,SACRyC,OAAQ,GACRV,MACAC,YACAC,SAIY,cAAVA,IACFS,EAAWD,OAASA,GAGtBpD,KAAKU,YAAY2C,IAGbC,EAAW,IAAIC,EAAAA,IAAapE,EAAW,CAC3CqE,aAAa,EACbC,qBAAqB,EACrBN,oBACAN,4BAGIa,GAAgBzC,EAAAA,EAAAA,IAAAA,EAAAA,EAAAA,GAAA,GACjBF,GAAM,IACT4C,gBAAiB5D,EACjB6D,WAAW,EACXC,MAAO,GACPC,YAAa,GACb5C,eAAgB,KAChBoC,WACAzD,oBACAkE,yBAAyB,SAKG9E,IAA1B8B,EAAOiD,uBACAN,EAAiBM,eAI5B,MAAM,gBAAEL,EAAe,UAAEM,SAAoB1E,EAAMyB,SAAS0C,GAC5D3D,EAAwB4D,EAMxB,IACIO,EADAC,EAJqBhF,EAAUiF,aAAaH,EAAW,CACzDR,qBAAqB,IAGkB,GAMzC,MAAMY,EAAyBlF,EAAUmF,OAAOvD,EAAOwD,UAAW,CAAEd,qBAAqB,IAErFU,EAAkBK,WAAWH,GAC7BH,EAAqBC,EAAkBM,UAAUJ,EAAuBrF,SAGxEkF,EAAqBC,EACrBnC,QAAQ0C,KAAK,sJAIjBR,EAAqBA,EAAmBS,QAAQ,yBAAyB,IAAIhD,OAE7E3B,KAAKU,YAAY,CACfC,OAAQ,uBACRyC,OAAQc,GAGZ,CAAE,MAAO7D,GACPL,KAAKU,YAAY,CACfC,OAAQ,QACRR,KAAM,sBACNC,KAAMC,EAAEO,YAEZ,CACF,CA+CMgE,CAAgBxE,GAChB,MAEF,IAAK,0BACHP,EAAkBgF,YAClB9E,EAAwB,KACxBC,KAAKU,YAAY,CAAEC,OAAQ,8BAC3B,MAEF,IAAK,sBACHZ,EAAwB,KACxBF,EAAkBuB,QAClBpB,KAAKU,YAAY,CAAEC,OAAQ,wBAC3B,MACF,QACEqB,QAAQ0C,KAAK,yDAA0DvE,M,GCxQzE2E,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqB/F,IAAjBgG,EACH,OAAOA,EAAaC,QAGrB,IAAIC,EAASL,EAAyBE,GAAY,CAGjDE,QAAS,CAAC,GAOX,OAHAE,EAAoBJ,GAAUG,EAAQA,EAAOD,QAASH,GAG/CI,EAAOD,OACf,CAGAH,EAAoBM,EAAID,EAGxBL,EAAoBjE,EAAI,KAGvB,IAAIwE,EAAsBP,EAAoBQ,OAAEtG,EAAW,CAAC,IAAI,MAAM,IAAO8F,EAAoB,QAEjG,OADAO,EAAsBP,EAAoBQ,EAAED,I,MChC7C,IAAIE,EAAW,GACfT,EAAoBQ,EAAI,CAACE,EAAQC,EAAUC,EAAIC,KAC9C,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASC,EAAI,EAAGA,EAAIP,EAASxG,OAAQ+G,IAAK,CACrCL,EAAWF,EAASO,GAAG,GACvBJ,EAAKH,EAASO,GAAG,GACjBH,EAAWJ,EAASO,GAAG,GAE3B,IAJA,IAGIC,GAAY,EACPC,EAAI,EAAGA,EAAIP,EAAS1G,OAAQiH,MACpB,EAAXL,GAAsBC,GAAgBD,IAAaM,OAAOC,KAAKpB,EAAoBQ,GAAGa,OAAOC,GAAStB,EAAoBQ,EAAEc,GAAKX,EAASO,MAC9IP,EAASY,OAAOL,IAAK,IAErBD,GAAY,EACTJ,EAAWC,IAAcA,EAAeD,IAG7C,GAAGI,EAAW,CACbR,EAASc,OAAOP,IAAK,GACrB,IAAIQ,EAAIZ,SACE1G,IAANsH,IAAiBd,EAASc,EAC/B,CACD,CACA,OAAOd,CArBP,CAJCG,EAAWA,GAAY,EACvB,IAAI,IAAIG,EAAIP,EAASxG,OAAQ+G,EAAI,GAAKP,EAASO,EAAI,GAAG,GAAKH,EAAUG,IAAKP,EAASO,GAAKP,EAASO,EAAI,GACrGP,EAASO,GAAK,CAACL,EAAUC,EAAIC,G,KCJ/Bb,EAAoByB,EAAI,CAACtB,EAASuB,KACjC,IAAI,IAAIJ,KAAOI,EACX1B,EAAoB2B,EAAED,EAAYJ,KAAStB,EAAoB2B,EAAExB,EAASmB,IAC5EH,OAAOS,eAAezB,EAASmB,EAAK,CAAEO,YAAY,EAAMC,IAAKJ,EAAWJ,MCJ3EtB,EAAoB+B,EAAI,CAAC,EAGzB/B,EAAoB1E,EAAK0G,GACjBpH,QAAQC,IAAIsG,OAAOC,KAAKpB,EAAoB+B,GAAGE,QAAO,CAACC,EAAUZ,KACvEtB,EAAoB+B,EAAET,GAAKU,EAASE,GAC7BA,IACL,KCNJlC,EAAoBmC,EAAKH,GAEjB,aAAeA,EAAU,IAAM,CAAC,IAAM,WAAW,IAAM,YAAYA,GAAW,YCFtFhC,EAAoBoC,SAAYJ,MCDhChC,EAAoB2B,EAAI,CAACU,EAAKC,IAAUnB,OAAOoB,UAAUC,eAAeC,KAAKJ,EAAKC,GCClFtC,EAAoBwB,EAAKrB,IACH,qBAAXuC,QAA0BA,OAAOC,aAC1CxB,OAAOS,eAAezB,EAASuC,OAAOC,YAAa,CAAEC,MAAO,WAE7DzB,OAAOS,eAAezB,EAAS,aAAc,CAAEyC,OAAO,KCLvD5C,EAAoB6C,EAAI,c,MCAxB7C,EAAoB8C,EAAI7H,KAAK8H,SAAW,aAIxC,IAAIC,EAAkB,CACrB,IAAK,GAkBNhD,EAAoB+B,EAAEf,EAAI,CAACgB,EAASE,KAE/Bc,EAAgBhB,IAElBiB,cAAcjD,EAAoB6C,EAAI7C,EAAoBmC,EAAEH,KAK/D,IAAIkB,EAAqBjI,KAA4B,sBAAIA,KAA4B,uBAAK,GACtFkI,EAA6BD,EAAmBE,KAAKC,KAAKH,GAC9DA,EAAmBE,KAzBC/H,IACnB,IAAIsF,EAAWtF,EAAK,GAChBiI,EAAcjI,EAAK,GACnBkI,EAAUlI,EAAK,GACnB,IAAI,IAAI4E,KAAYqD,EAChBtD,EAAoB2B,EAAE2B,EAAarD,KACrCD,EAAoBM,EAAEL,GAAYqD,EAAYrD,IAIhD,IADGsD,GAASA,EAAQvD,GACdW,EAAS1G,QACd+I,EAAgBrC,EAAS6C,OAAS,EACnCL,EAA2B9H,G,WCrB5B,IAAIoI,EAAOzD,EAAoBjE,EAC/BiE,EAAoBjE,EAAI,IAChBnB,QAAQC,IAAI,CAClBmF,EAAoB1E,EAAE,KACtB0E,EAAoB1E,EAAE,OACpBoI,KAAKD,E,KCJiBzD,EAAoBjE,G","sources":["summarization.worker.js","../webpack/bootstrap","../webpack/runtime/chunk loaded","../webpack/runtime/define property getters","../webpack/runtime/ensure chunk","../webpack/runtime/get javascript chunk filename","../webpack/runtime/get mini-css chunk filename","../webpack/runtime/hasOwnProperty shorthand","../webpack/runtime/make namespace object","../webpack/runtime/publicPath","../webpack/runtime/importScripts chunk loading","../webpack/runtime/startup chunk dependencies","../webpack/startup"],"sourcesContent":["/* eslint-disable no-restricted-globals */\nimport {\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  TextStreamer,\n  InterruptableStoppingCriteria,\n} from \"@huggingface/transformers\";\n\n/**\n * Helper function to perform feature detection for WebGPU\n */\nasync function checkWebGPU() {\n  try {\n    if (!navigator.gpu) {\n      throw new Error(\"WebGPU is not supported by this browser.\");\n    }\n    const adapter = await navigator.gpu.requestAdapter();\n    if (!adapter) {\n      throw new Error(\"WebGPU is not supported (no adapter found)\");\n    }\n    // fp16_supported = adapter.features.has(\"shader-f16\") // Example: check for specific features\n    self.postMessage({\n      status: \"webgpu_check_success\",\n      data: \"WebGPU is available.\",\n    });\n  } catch (e) {\n    self.postMessage({\n      status: \"error\",\n      type: \"webgpu_check_error\",\n      data: e.toString(),\n    });\n  }\n}\n\n/**\n * This class uses the Singleton pattern to enable lazy-loading of the pipeline\n */\nclass SummarizationPipeline {\n  static model_id = \"onnx-community/Qwen3-0.6B-ONNX\"; // Using the example model\n  static tokenizer = null;\n  static model = null;\n\n  static async getInstance(progress_callback = null) {\n    if (!this.tokenizer) {\n      this.tokenizer = AutoTokenizer.from_pretrained(this.model_id, {\n        progress_callback,\n      });\n    }\n\n    if (!this.model) {\n      this.model = AutoModelForCausalLM.from_pretrained(this.model_id, {\n        dtype: \"q4f16\", // Using quantization for better performance\n        device: \"webgpu\",\n        progress_callback,\n      });\n    }\n\n    // Wait for both promises to resolve\n    [this.tokenizer, this.model] = await Promise.all([this.tokenizer, this.model]);\n    return [this.tokenizer, this.model];\n  }\n}\n\nconst stopping_criteria = new InterruptableStoppingCriteria();\nlet past_key_values_cache = null;\n\n// reasonEnabled parameter is removed, thinking mode is now default\nasync function generateSummary({ transcriptionText, meetingTitle }) { // Added meetingTitle\n  self.postMessage({ status: \"summarizing_started\", data: { reasonEnabled: true } }); // Always indicate reasonEnabled is true\n\n  try {\n    const [tokenizer, model] = await SummarizationPipeline.getInstance((progress) => {\n      self.postMessage(progress); // Forward progress events\n    });\n\n    // Construct the user content with the meeting title\n    let userContent = `Summarize the following meeting transcript concisely, focusing on key decisions, action items, and important discussions.`;\n    if (meetingTitle && meetingTitle.trim() !== \"\") {\n      userContent += ` The title of this meeting is \"${meetingTitle}\".`;\n    }\n    userContent += ` Structure the summary with the following sections:\\n\\n1. Meeting Overview – Date, attendees, and primary objective.\\n2. Key Discussion Points – Bullet points of major topics covered (limit to 3-5).\\n3. Decisions Made – Clear outcomes or resolutions agreed upon.\\n4. Action Items – Specific tasks, assigned owners, and deadlines (if mentioned).\\n5. Next Steps – Any follow-up meetings or pending discussions.\\nMaintain a professional tone, avoid unnecessary details, and ensure clarity. Here is the transcript:\\n\\n\"${transcriptionText}\"`;\n    \n    // Always use chat template for thinking mode\n    const messages = [\n      { role: \"system\", content: \"You are a helpful meeting summarization assistant. First, think about the key points of the transcription, then provide a concise summary.\" },\n      { role: \"user\", content: userContent }\n    ];\n\n    console.log(messages)\n    const inputs = tokenizer.apply_chat_template(messages, {\n      add_generation_prompt: true,\n      return_dict: true,\n    });\n\n    // Always encode <think> tokens\n    const [START_THINKING_TOKEN_ID, END_THINKING_TOKEN_ID] = tokenizer.encode(\n      \"<think></think>\",\n      { add_special_tokens: false },\n    );\n\n    let startTime;\n    let numTokens = 0;\n    let tps;\n    let state = \"answering\"; // 'thinking' or 'answering'\n\n    const token_callback_function = (tokens) => {\n      startTime ??= performance.now();\n      if (numTokens++ > 0) {\n        tps = (numTokens / (performance.now() - startTime)) * 1000;\n      }\n      // Always check for think tokens\n      switch (Number(tokens[0])) {\n        case START_THINKING_TOKEN_ID:\n          state = \"thinking\";\n          break;\n        case END_THINKING_TOKEN_ID:\n          state = \"answering\";\n          break;\n      }\n      // console.log(state, tokens, tokenizer.decode(tokens));\n    };\n\n    const callback_function = (output) => {\n      let dataToSend = {\n        status: \"update\",\n        output: \"\", // Default to empty output\n        tps,\n        numTokens,\n        state,\n      };\n\n      // Always apply thinking mode logic for streaming\n      if (state === \"answering\") {\n        dataToSend.output = output; // Send raw output during answering state\n      }\n      // If state is \"thinking\", output remains empty. App.js shows status based on state.\n      self.postMessage(dataToSend);\n    };\n\n    const streamer = new TextStreamer(tokenizer, {\n      skip_prompt: true,\n      skip_special_tokens: true,\n      callback_function,\n      token_callback_function,\n    });\n\n    const generationConfig = {\n      ...inputs,\n      past_key_values: past_key_values_cache,\n      do_sample: true,\n      top_k: 20, // Always use thinking mode parameters\n      temperature: 0.6, // Always use thinking mode parameters\n      max_new_tokens: 1024, // Always use thinking mode parameters\n      streamer,\n      stopping_criteria,\n      return_dict_in_generate: true,\n    };\n    \n    // For some models, token_type_ids might not be needed or might cause issues if not handled correctly.\n    // If the model is not a BERT-style model, it's often safer to omit token_type_ids.\n    if (inputs.token_type_ids === undefined) {\n        delete generationConfig.token_type_ids;\n    }\n\n\n    const { past_key_values, sequences } = await model.generate(generationConfig);\n    past_key_values_cache = past_key_values;\n\n    const decodedSequences = tokenizer.batch_decode(sequences, {\n      skip_special_tokens: true, // This primarily removes tokenizer-defined special tokens like <s>, </s>\n    });\n    \n    let fullGeneratedText = decodedSequences[0]; // This text includes the prompt if not skipped by batch_decode\n    let cleanedFinalOutput;\n\n    // Always apply thinking mode cleaning logic\n    // `inputs.input_ids` contains the tokenized version of the full chat prompt + assistant generation hint\n    // Decode these input_ids to get the exact prompt text that was fed to the model.\n    const promptTextFromInputIds = tokenizer.decode(inputs.input_ids, { skip_special_tokens: true });\n\n    if (fullGeneratedText.startsWith(promptTextFromInputIds)) {\n        cleanedFinalOutput = fullGeneratedText.substring(promptTextFromInputIds.length);\n    } else {\n        // Fallback\n        cleanedFinalOutput = fullGeneratedText;\n        console.warn(\"Summarization worker: Prompt (from input_ids) not found at the start of batch_decode output. The final summary might contain parts of the prompt.\");\n    }\n    \n    // Remove <think>...</think> blocks from the assistant's actual response part.\n    cleanedFinalOutput = cleanedFinalOutput.replace(/<think>.*?<\\/think>/gs, \"\").trim();\n\n    self.postMessage({\n      status: \"summarizing_complete\",\n      output: cleanedFinalOutput,\n    });\n\n  } catch (e) {\n    self.postMessage({\n      status: \"error\",\n      type: \"summarization_error\",\n      data: e.toString(),\n    });\n  }\n}\n\nasync function loadModel() {\n  self.postMessage({\n    status: \"loading\",\n    data: \"Loading summarization model...\",\n  });\n\n  try {\n    const [tokenizer, model] = await SummarizationPipeline.getInstance((x) => {\n      self.postMessage(x);\n    });\n\n    self.postMessage({\n      status: \"loading\",\n      data: \"Compiling shaders and warming up summarization model...\",\n    });\n\n    // Run model with dummy input to compile shaders\n    const dummyPrompt = \"This is a test.\";\n    const inputs = tokenizer(dummyPrompt);\n    await model.generate({ ...inputs, max_new_tokens: 1 });\n    self.postMessage({ status: \"summarization_ready\" });\n  } catch (e) {\n    self.postMessage({\n      status: \"error\",\n      type: \"load_model_error\",\n      data: e.toString(),\n    });\n  }\n}\n\n// Listen for messages from the main thread\nself.addEventListener(\"message\", async (e) => {\n  const { type, data } = e.data;\n\n  switch (type) {\n    case \"check_webgpu\":\n      checkWebGPU();\n      break;\n\n    case \"load_model\":\n      loadModel();\n      break;\n\n    case \"summarize\":\n      stopping_criteria.reset();\n      generateSummary(data); // data should be { transcriptionText: \"...\" }\n      break;\n\n    case \"interrupt_summarization\":\n      stopping_criteria.interrupt();\n      past_key_values_cache = null; // Clear cache on interrupt\n      self.postMessage({ status: \"summarization_interrupted\" });\n      break;\n\n    case \"reset_summarization\":\n      past_key_values_cache = null;\n      stopping_criteria.reset();\n      self.postMessage({ status: \"summarization_reset\" });\n      break;\n    default:\n      console.warn(\"Unknown message type received in summarization worker:\", type);\n      break;\n  }\n});\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n// the startup function\n__webpack_require__.x = () => {\n\t// Load entry module and return exports\n\t// This entry module depends on other loaded chunks and execution need to be delayed\n\tvar __webpack_exports__ = __webpack_require__.O(undefined, [827,397], () => (__webpack_require__(7575)))\n\t__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n\treturn __webpack_exports__;\n};\n\n","var deferred = [];\n__webpack_require__.O = (result, chunkIds, fn, priority) => {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every((key) => (__webpack_require__.O[key](chunkIds[j])))) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.f = {};\n// This file contains only the entry chunk.\n// The chunk loading function for additional chunks\n__webpack_require__.e = (chunkId) => {\n\treturn Promise.all(Object.keys(__webpack_require__.f).reduce((promises, key) => {\n\t\t__webpack_require__.f[key](chunkId, promises);\n\t\treturn promises;\n\t}, []));\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.u = (chunkId) => {\n\t// return url for filenames based on template\n\treturn \"static/js/\" + chunkId + \".\" + {\"397\":\"1823f431\",\"827\":\"d8841902\"}[chunkId] + \".chunk.js\";\n};","// This function allow to reference async chunks and sibling chunks for the entrypoint\n__webpack_require__.miniCssF = (chunkId) => {\n\t// return url for filenames based on template\n\treturn undefined;\n};","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.p = \"/noted-pak/\";","__webpack_require__.b = self.location + \"/../../../\";\n\n// object to store loaded chunks\n// \"1\" means \"already loaded\"\nvar installedChunks = {\n\t575: 1\n};\n\n// importScripts chunk loading\nvar installChunk = (data) => {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\tfor(var moduleId in moreModules) {\n\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t}\n\t}\n\tif(runtime) runtime(__webpack_require__);\n\twhile(chunkIds.length)\n\t\tinstalledChunks[chunkIds.pop()] = 1;\n\tparentChunkLoadingFunction(data);\n};\n__webpack_require__.f.i = (chunkId, promises) => {\n\t// \"1\" is the signal for \"already loaded\"\n\tif(!installedChunks[chunkId]) {\n\t\tif(true) { // all chunks have JS\n\t\t\timportScripts(__webpack_require__.p + __webpack_require__.u(chunkId));\n\t\t}\n\t}\n};\n\nvar chunkLoadingGlobal = self[\"webpackChunknoted_pak\"] = self[\"webpackChunknoted_pak\"] || [];\nvar parentChunkLoadingFunction = chunkLoadingGlobal.push.bind(chunkLoadingGlobal);\nchunkLoadingGlobal.push = installChunk;\n\n// no HMR\n\n// no HMR manifest","var next = __webpack_require__.x;\n__webpack_require__.x = () => {\n\treturn Promise.all([\n\t\t__webpack_require__.e(827),\n\t\t__webpack_require__.e(397)\n\t]).then(next);\n};","// run startup\nvar __webpack_exports__ = __webpack_require__.x();\n"],"names":["SummarizationPipeline","getInstance","progress_callback","arguments","length","undefined","this","tokenizer","AutoTokenizer","from_pretrained","model_id","model","AutoModelForCausalLM","dtype","device","Promise","all","stopping_criteria","InterruptableStoppingCriteria","past_key_values_cache","self","addEventListener","async","type","data","e","navigator","gpu","Error","requestAdapter","postMessage","status","toString","checkWebGPU","x","inputs","generate","_objectSpread","max_new_tokens","loadModel","reset","_ref","transcriptionText","meetingTitle","reasonEnabled","progress","userContent","trim","concat","messages","role","content","console","log","apply_chat_template","add_generation_prompt","return_dict","START_THINKING_TOKEN_ID","END_THINKING_TOKEN_ID","encode","add_special_tokens","startTime","tps","numTokens","state","token_callback_function","tokens","_startTime","performance","now","Number","callback_function","output","dataToSend","streamer","TextStreamer","skip_prompt","skip_special_tokens","generationConfig","past_key_values","do_sample","top_k","temperature","return_dict_in_generate","token_type_ids","sequences","cleanedFinalOutput","fullGeneratedText","batch_decode","promptTextFromInputIds","decode","input_ids","startsWith","substring","warn","replace","generateSummary","interrupt","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","exports","module","__webpack_modules__","m","__webpack_exports__","O","deferred","result","chunkIds","fn","priority","notFulfilled","Infinity","i","fulfilled","j","Object","keys","every","key","splice","r","d","definition","o","defineProperty","enumerable","get","f","chunkId","reduce","promises","u","miniCssF","obj","prop","prototype","hasOwnProperty","call","Symbol","toStringTag","value","p","b","location","installedChunks","importScripts","chunkLoadingGlobal","parentChunkLoadingFunction","push","bind","moreModules","runtime","pop","next","then"],"sourceRoot":""}